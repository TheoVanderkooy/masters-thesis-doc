
\chapter{\label{sec:pbm-pq_postgres}Implementation in P\MakeLowercase{ostgre}SQL}


This chapter describes the implementation of predictive buffer management in PostgreSQL. This includes implementing both the sampling-based approach and the PQ-based approach that is not implemented in an open source system, into PostgreSQL. Having both implementations in PostgreSQL allows us to conduct an apples-to-apples evaluation and  comparison of the two approaches. The differences between the implementation in \cite{pbm} and this thesis will be pointed out in the relevant parts of this chapter.

%Note that due to differences between PostgreSQL and Actian Vector, there will be certain differences between the PostgreSQL implementation of PBM-PQ and what would have been implemented by \cite{pbm}. These differences will be pointed out in the relevant parts of this chapter.


\section{PostgreSQL's Existing Buffer Replacement}

As previously mentioned, PostgreSQL uses a clock-sweep strategy for cache replacement. PostgreSQL has an in-memory array of buffer headers -- separate from the buffer contents -- that include a usage count used by clock-sweep as well as other fields, including a reference count, identifier of which block is in the buffer, and whether it needs to be written back to secondary storage among others.

When a buffer is allocated, the system first checks a linked list of free buffers, and if that is empty it runs the clock-sweep algorithm. The clock-sweep strategy involves reading and atomically incrementing a global index into the array of buffer headers (the ``clock hand''), and checking the buffer at that index. If the buffer has a zero usage count and is not in use, it is selected for replacement with the new buffer. Otherwise, its usage count is decremented and the process is repeated until a suitable buffer is chosen for replacement.

PostgreSQL's cache replacement selection is very simple, with only a few global variables, a global linked list of free buffers, and an extra field in each buffer header. To support a predictive buffer management strategy, some changes are required to track the extra metadata.


\section{Changes to PostgreSQL to Support PBM}

First, some additional data structures are required. This includes a set of the active scans and a hash map to track the set of registered scans for each block in the database, as well as some global metadata and other data structures for managing the shared memory used by the \gls{pbm} data structures. The hash map of blocks additionally has its entries linked together in order by block number so that initially registering a scan requires only a single hash lookup to register with all blocks. The PostgreSQL implementation of PBM-PQ additionally adds the approximate priority queue as a global data structure. The existing buffer headers have some additional fields added to track frequency statistics for cached blocks, and pointers to the relevant entries in the other shared data structures to facilitate low-latency computation of next-access estimates. 

Scan operators are also augmented with extra fields to track their progress -- both current position and speed -- and a pointer to the corresponding structure in shared \gls{pbm} metadata used by access-time estimates. When a sequential scan first starts it uses the hash map of blocks to find the blocks it must register with. At run-time, scans update their local statistics as they go and periodically update the shared statistics to keep them up-to-date, as well as un-registering the scan from blocks that have been processed and will not be needed again by that scan.


\subsection{Details of PBM implementation}

This sub-chapter provides more details on specific aspects of the PostgreSQL implementation.


\textbf{Block groups:} Both \gls{pbm} approaches store metadata about active scans for every block in the database. With PostgreSQL's default 8~KiB block size, even a few tens of bytes per requested block per scan will require this metadata to consume significant amounts of memory for a large database when many scans are active. To reduce the memory footprint, consecutive blocks of each relation are grouped into \textit{block groups} of a constant size and \gls{pbm} metadata for sequential scans is stored at the block group level instead of for every individual block. The PBM-sampling implementation uses a default block group size of 1~MiB, so 128 consecutive blocks share metadata about requesting scans. 1~MiB is chosen partially to correspond with the granularity of PostgreSQL BRIN indexes, which by default also store statistics about 1~MiB chunks of data. The original PBM-PQ implementation~\cite{pbm} would not need such block groups as they already uses a much larger default block size. \cite{vectorDocs}  % ... block groups get stored in the PQ buckets, not the buffers directly

The block group statistics are stored in a hash map and block groups for the same table form a linked list such that they can be traversed sequentially without multiple hash table look-ups when registering a sequential scan. % this is simplified: the block groups are actually allocated in fixed-size chunks and those chunks are linked together. This saves a bit of memory and reduces pointer chasing when scanning through the block groups, though it is probably insignificant.
When a block is loaded into the cache, the hash map is used to find the associated block group. A pointer to the block group is stored in the buffer header to avoid hash look-ups later when estimating the next access time of the cached block. For the PQ-based implementation the block group must also store a list of currently cached blocks from the group, so that the actual blocks can be evicted from the cache when the block group is chosen for eviction. 

% (for sampling: block->group pointer is used to estimate access time of a block. For pq-based: it is used to remove the buffer from the group later when buffer is evicted)


\textbf{Bulk eviction:} \citet{pbm} state that their implementation of PBM-PQ evicts ``groups of 16 or more'' buffers at once to amortize eviction cost. In the PostgreSQL implementation of PBM-PQ, all buffers from the relevant bucket are selected for eviction, which are then placed on PostgreSQL's existing free-list to be replaced as needed. This decision was made primarily to reduce complexity of the eviction method. On sequential workloads where items will be fairly evenly distributed among the different buckets this should not have much impact, but it will change the behaviour on non-sequential workloads where the approximate priority queue classifies everything as not requested: the description from \cite{pbm} would behave like \gls{lru} in this case while the PostgreSQL implementation will be more similar to \gls{fifo}.

The sampling-based approach does not need to amortize eviction cost, which is already very low with a small sample size. There are some benefits to bulk eviction as discussed in Chapter~\ref{sec:sampling-bulk-eviction} but with the default configuration, PBM-sampling evicts only a single buffer at once.

% reason for PBM-PQ evicting a whole bucket at once:
%  - the block-group makes it more complicated to only evict part of the bucket. Some blocks from the bucket could be pinned preventing the group from getting removed from the bucket.
%  - It also is not evicted immediately, only placed onto the free list, so over-evicting is not a big concern since they are still usable until the space is actually needed.


\textbf{Scan ranges and bitmap scans:} As discussed in Chapter~\ref{sec:bitmap_scans}, the PostgreSQL implementation handles min-max indexes by tracking all bitmap scans in a manner very similar to sequential scans. This implicitly supports other bitmap-only indexes and scenarios where a bitmap scan is used for other index types that \cite{pbm} does not handle. The \gls{pbm} implementation treats bitmap scans much like sequential scans, except the bitmap is used to determine which block will be accessed and should or should not be registered.


\textbf{Index scans:} Unlike sequential and bitmap scans, where the scan is registered with each block group, non-bitmap index scans are registered once and store statistics about the scan in a hash map keyed by table ID with a list of active scans for each table. When a block is loaded into cache, a reference to the list of index scans on the relevant table is stored in the buffer header to avoid hash look-ups each time the next-access-time is recalculated.


\textbf{Frequency statistics:} The average inter-access time is tracked in new fields added to the buffer headers. These fields are updated when a block is requested and found to be already present in the buffer cache.


\textbf{Shared memory and concurrency:} A challenge in the PostgreSQL implementation of both forms of \gls{pbm} is dealing with PostgreSQL's shared memory implementation. PostgreSQL uses separate processes for each client connection, and more than one process for the same client if a parallel query plan is used. Using separate processes instead of separate threads in the same process makes sharing the \gls{pbm} data structures between all parallel tasks difficult. Shared data structures in PostgreSQL must be allocated in shared memory when the database is started, limiting the ability to scale the memory used by \gls{pbm} data structures. The implementation deals with this case by over-provisioning shared memory for \gls{pbm} data structures. When bits of shared memory is no longer needed, such as when a scan completes and its \gls{pbm} metadata could be freed, the \gls{pbm} implementation keeps track of the old allocations to be reused by the next scan.
%This is a limitation of the PostgreSQL implementation that should not be an issue in newer systems running in a single process.


% \textbf{other system-level differences:} column store vs not, synchronized sequential scans, ...

